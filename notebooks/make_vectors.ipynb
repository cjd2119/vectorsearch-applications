{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp-5rNMUE0OF",
        "outputId": "e11b74fe-25cb-47f5-fa30-ac8b00d715ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# standard libraries\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "import os\n",
        "import tiktoken\n",
        "import time\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "from math import ceil\n",
        "\n",
        "# external libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from llama_index.text_splitter import SentenceSplitter  # one of the best on the market\n",
        "from rich import print\n",
        "from rich.pretty import pprint  # nifty library for pretty printing\n",
        "from sentence_transformers import SentenceTransformer, losses, InputExample, models\n",
        "from torch import cuda\n",
        "from tqdm import tqdm\n",
        "\n",
        "# external files\n",
        "try:\n",
        "    from preprocessing import FileIO\n",
        "except ModuleNotFoundError:\n",
        "    from src.preprocessor.preprocessing import FileIO\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "from src.database.weaviate_interface_v4 import WeaviateIndexer, WeaviateWCS\n",
        "from src.database.database_utils import get_weaviate_client\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "from rich import (\n",
        "    print,\n",
        ")  # nice library that provides improved printing output (overrides default print function)\n",
        "\n",
        "from src.database.properties_template import properties\n",
        "\n",
        "api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
        "url = os.environ[\"WEAVIATE_ENDPOINT\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oY7nFzmBFPWy"
      },
      "outputs": [],
      "source": [
        "def split_contents(\n",
        "    corpus: list[dict], text_splitter: SentenceSplitter, content_field: str = \"content\"\n",
        ") -> list[list[str]]:\n",
        "    \"\"\"\n",
        "    Given a corpus of \"documents\" with text content, this function splits the\n",
        "    content field into chunks sizes as specified by the text_splitter.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    corpus = [\n",
        "            {'title': 'This is a cool show', 'content': 'There is so much good content on this show. \\\n",
        "              This would normally be a really long block of content. ... But for this example it will not be.'},\n",
        "            {'title': 'Another Great Show', 'content': 'The content here is really good as well.  If you are \\\n",
        "              reading this you have too much time on your hands. ... More content, blah, blah.'}\n",
        "           ]\n",
        "\n",
        "    output = split_contents(data, text_splitter, content_field=\"content\")\n",
        "\n",
        "    output >>> [['There is so much good content on this show.', 'This would normally be a really long block of content.', \\\n",
        "                 'But for this example it will not be'],\n",
        "                ['The content here is really good as well.', 'If you are reading this you have too much time on your hands.', \\\n",
        "                 'More content, blah, blah.']\n",
        "                ]\n",
        "    \"\"\"\n",
        "\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "    output = []\n",
        "    for doc in tqdm(corpus):\n",
        "        output.append(text_splitter.split_text(doc[content_field]))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def encode_content_splits(\n",
        "    content_splits: list[list[str]],\n",
        "    model: SentenceTransformer,\n",
        "    device: str = \"cuda:0\" if cuda.is_available() else \"cpu\",\n",
        ") -> list[list[tuple[str, list[float]]]]:\n",
        "    \"\"\"\n",
        "    Encode content splits as vector embeddings from a vectors of content splits\n",
        "    where each vectors of splits is a single podcast episode.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    content_splits =  [['There is so much good content on this show.', 'This would normally be a really long block of content.'],\n",
        "                       ['The content here is really good as well.', 'More content, blah, blah.']\n",
        "                      ]\n",
        "\n",
        "    output = encode_content_splits(content_splits, model)\n",
        "\n",
        "    output >>> [\n",
        "          EPISODE 1 -> [('There is so much good content on this show.',[ 1.78036056e-02, -1.93265956e-02,  3.61164124e-03, -5.89650944e-02,\n",
        "                                                                         1.91510320e-02,  1.60808843e-02,  1.13610983e-01,  3.59948091e-02,\n",
        "                                                                        -1.73066761e-02, -3.30348089e-02, -1.00898169e-01,  2.34847311e-02]\n",
        "                                                                        )\n",
        "                         tuple(text, vectors), tuple(text, vectors), tuple(text, vectors)....],\n",
        "          EPISODE 2 ->  [tuple(text, vectors), tuple(text, vectors), tuple(text, vectors)....],\n",
        "          EPISODE n ... [tuple(text, vectors), tuple(text, vectors), tuple(text, vectors)....]\n",
        "    \"\"\"\n",
        "\n",
        "    text_vector_tuples = []\n",
        "\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for content in tqdm(content_splits):\n",
        "        vecs = model.encode(content).tolist()\n",
        "        text_vector = [(t, v) for t, v in zip(content, vecs)]\n",
        "        text_vector_tuples.append(text_vector)\n",
        "\n",
        "    return text_vector_tuples\n",
        "\n",
        "\n",
        "def join_metadata(\n",
        "    corpus: list[dict],\n",
        "    text_vector_list: list[list[tuple[str, list]]],\n",
        "    unique_id_field: str = \"video_id\",\n",
        "    content_field: str = \"content\",\n",
        "    embedding_field: str = \"content_embedding\",\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Combine episode metadata from original corpus with text/vectors tuples.\n",
        "    Creates a new dictionary for each text/vector combination.\n",
        "    \"\"\"\n",
        "\n",
        "    joined_documents = []\n",
        "\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "\n",
        "    for i, doc in enumerate(corpus):\n",
        "        for j, tv in enumerate(text_vector_list[i]):\n",
        "            corp_dict = {key: value for key, value in doc.items() if key != \"content\"}\n",
        "            video_id = doc[\"video_id\"]\n",
        "            corp_dict[\"doc_id\"] = f\"{video_id}_{j}\"\n",
        "            corp_dict[\"content\"] = tv[0]\n",
        "            corp_dict[\"content_embedding\"] = tv[1]\n",
        "            joined_documents.append(corp_dict)\n",
        "\n",
        "    return joined_documents\n",
        "\n",
        "\n",
        "def create_dataset(\n",
        "    corpus: list[dict],\n",
        "    embedding_model: SentenceTransformer,\n",
        "    text_splitter: SentenceSplitter,\n",
        "    save_to_disk: bool,\n",
        "    file_outpath: str = None,\n",
        "    unique_id_field: str = \"video_id\",\n",
        "    content_field: str = \"content\",\n",
        "    embedding_field: str = \"content_embedding\",\n",
        "    device: str = \"cuda:0\" if cuda.is_available() else \"cpu\",\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Given a raw corpus of data, this function creates a new dataset where each dataset\n",
        "    doc contains episode metadata and it's associated text chunk and vector representation.\n",
        "    Output is directly saved to disk.\n",
        "    \"\"\"\n",
        "    if save_to_disk and not file_outpath:\n",
        "        raise ValueError(\n",
        "            f\"Saving to disk is enabled but file_outpath was left as a None value.\\n\\\n",
        "            Enter a valid file_outpath or mark save_to_disk as False\"\n",
        "        )\n",
        "\n",
        "    io = FileIO()\n",
        "\n",
        "    chunk_size = text_splitter.chunk_size\n",
        "    print(f\"Creating dataset using chunk_size: {chunk_size}\")\n",
        "    start = time.perf_counter()\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "    content_splits = split_contents(corpus, text_splitter)\n",
        "    text_vector_tuples = encode_content_splits(content_splits, embedding_model)\n",
        "    joined_docs = join_metadata(corpus, text_vector_tuples)\n",
        "    ########################\n",
        "    # END YOUR CODE HERE #\n",
        "    ########################\n",
        "    if save_to_disk:\n",
        "        io.save_as_parquet(file_path=file_outpath, data=joined_docs, overwrite=False)\n",
        "    end = time.perf_counter() - start\n",
        "    print(\n",
        "        f\"Total Time to process dataset of chunk_size ({chunk_size}): {round(end/60, 2)} minutes\"\n",
        "    )\n",
        "    return joined_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jCukWhaPE4MN"
      },
      "outputs": [],
      "source": [
        "# root folder on Google Colab is: /content/\n",
        "root_folder = \"../data/\"\n",
        "data_file = \"huberman_labs.json\"\n",
        "data_path = os.path.join(root_folder, data_file)\n",
        "data_path\n",
        "\n",
        "data = FileIO.load_json(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pretrained_model(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "    \"\"\"\n",
        "    Loads sentence transformer modules and returns a pretrained\n",
        "    model for finetuning.\n",
        "    \"\"\"\n",
        "    word_embedding_model = models.Transformer(model_name_or_path=model_name)\n",
        "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "    model.to(\"cuda\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cuI6QsBJE7TS"
      },
      "outputs": [],
      "source": [
        "# define the model you want to use\n",
        "model_names = [\n",
        "    \"../models/bge-base-finetuned-500\",\n",
        "]\n",
        "\n",
        "base = [\"bge_finetuned_500\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "SIyEDawIFGUH",
        "outputId": "ad1b397e-c8ec-4b04-be2d-933038da30a4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating dataset using chunk_size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Creating dataset using chunk_size: \u001b[1;36m512\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [00:18<00:00, 10.50it/s]\n",
            "100%|██████████| 193/193 [06:37<00:00,  2.06s/it]\n",
            "\u001b[32m2024-05-21 01:53:46.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msave_as_parquet\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mDataFrame saved as parquet file here: ../data/huberman_bge_finetuned_500_512.parquet\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Time to process dataset of chunk_size <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.95</span> minutes\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Total Time to process dataset of chunk_size \u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m6.95\u001b[0m minutes\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of data: (11602, 13)\n",
            "Memory Usage: 1.15+ MB\n",
            "Collection \"Huberman_bge_finetuned_500_512\" created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/tmp/ipykernel_4024/3092884582.py:29: ResourceWarning: unclosed <ssl.SSLSocket fd=78, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.162.0.2', 39158), raddr=('34.149.137.116', 443)>\n",
            "  indexer = WeaviateIndexer(client)\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "100%|██████████| 11602/11602 [00:19<00:00, 595.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing finished in 0.58 minutes.\n",
            "Batch job completed with zero errors.\n"
          ]
        }
      ],
      "source": [
        "chunk_sizes = [512]\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "client = WeaviateWCS(endpoint=url, api_key=api_key, model_name_or_path=model_name)\n",
        "\n",
        "for chunk_size in chunk_sizes:\n",
        "    for model_name, bas in zip(model_names, base):\n",
        "        gpt35_txt_splitter = SentenceSplitter(\n",
        "            chunk_size=chunk_size, tokenizer=encoding.encode, chunk_overlap=0\n",
        "        )\n",
        "        outpath = f\"../data/huberman_{bas}_{chunk_size}\"\n",
        "        model = load_pretrained_model(model_name)\n",
        "        create_dataset(\n",
        "            data, model, gpt35_txt_splitter, save_to_disk=True, file_outpath=outpath\n",
        "        )\n",
        "\n",
        "        data_path = f\"../data/huberman_{bas}_{chunk_size}.parquet\"\n",
        "\n",
        "        data_pqt = FileIO.load_parquet(data_path)\n",
        "\n",
        "        collection_name = f\"Huberman_{bas}_{chunk_size}\"\n",
        "\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            properties=properties,\n",
        "            description=\"Huberman Labs: 193 full-length transcripts\",\n",
        "        )\n",
        "\n",
        "        indexer = WeaviateIndexer(client)\n",
        "\n",
        "        batch_object = indexer.batch_index_data(data_pqt, collection_name)\n",
        "\n",
        "client.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
